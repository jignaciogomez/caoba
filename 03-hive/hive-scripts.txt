# Hadoop - HDFS
# Curso Citizen Data Scientist - CAOBA
# Profesor: Edwin Montoya M. – emontoya@eafit.edu.co
# 2018
#
# Scripts de HIVE
#

- PREPARACIÓN DE DATOS:

* preparar los datos de trabajo para hive:

    $ hdfs dfs -mkdir /user/<username>/hive_tmp/
    $ hdfs dfs -chmod 777 /user/<username>/hive_tmp/
    $ hdfs dfs -put hdi-data.csv /user/<username>/hive_tmp/
    $ hdfs dfs -chmod 777  /user/<username>/datasets/hdi-data.csv

    $ hdfs dfs -mkdir /user/<username>/hive_tmp/gutenberg/
    $ hdfs dfs -chmod 777 /user/<username>/hive_tmp/gutenberg/
    $ hdfs dfs -put gutenberg/*.txt /user/<username>/hive_tmp/gutenberg/
    $ hdfs dfs -chmod 777 /user/<username>/hive_tmp/gutenberg/*

- HIVE

*conectar al cluster-hive:

    $ beeline
    beeline>!connet jdbc:hive2://hpcdis.dis.eafit.edu.co:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2

*crear una base de datos:

    beeline> create database <username>;
    beeline> use <username>;

*crear una tabla MANEJADA en la base de datos <username>

    beeline> use <username>;
    beeline> CREATE TABLE HDI (id INT, country STRING, hdi FLOAT, lifeex INT, mysch INT, eysch INT, gni INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

*cargar los datos LOCALES a la tabla HDI:

    beeline> use <username>;
    beeline> load data local inpath '/home/<username>/datasets/hdi-data.csv' into table HDI;

*cargar los datos desde HDFS a la tabla HDI:

    beeline> load data inpath '/user/<username>/datasets/hdi-data.csv' overwrite into table HDI;
    
*describir una tabla:

    beeline> use <username>;
    beeline>describe hdi;

*hacer consultas y cálculos sobre la tabla HDI:

    beeline> use <username>;
    beeline> select * from hdi; 
    beeline> select country, gni from hdi where gni > 2000; 

*WORDCOUNT EN HIVE:

* crear tabla
    beeline> use <username>;
    beeline> CREATE TABLE docs (line STRING); 

* carga incremental 
    beeline> LOAD DATA INPATH '<dir>/gutenberg/*.txt' INTO TABLE docs; 

* carga desde cero 
    beeline> LOAD DATA INPATH '<dir>/gutenberg/*.txt' OVERWRITE INTO TABLE docs; 

* ordenado por palabra 
    beeline> SELECT word, count(1) AS count FROM (SELECT explode(split(line,' ')) AS word FROM docs) w GROUP BY word ORDER BY word; 

* ordenado por frecuencia de menor a mayor 
    beeline> SELECT word, count(1) AS count FROM (SELECT explode(split(line,' ')) AS word FROM docs) w GROUP BY word ORDER BY count desc;

- SQOOP

- MYSQL

CREATE USER 'curso'@'%' IDENTIFIED BY 'curso';

GRANT ALL PRIVILEGES ON cursodb.* TO 'curso'@'%';

insert into employee values (101, 'name1', 1800);
insert into employee values (102, 'name2', 1500);
insert into employee values (103, 'name3', 1000);
insert into employee values (104, 'name4', 2000);
insert into employee values (105, 'name5', 1600);

sqoop import --connect jdbc:mysql://192.168.10.80:3306/cursodb --username curso -P --table employee --target-dir /user/emontoya/mysqlOut1 -m 1

sqoop create-hive-table --connect jdbc:mysql://192.168.10.80:3306/cursodb --username curso -P --database emontoya --hive-database emontoya --table employee --hive-table empleados --mysql-delimiters

sqoop import --connect jdbc:mysql://192.168.10.80:3306/cursodb --username curso -P --hive-database emontoya --table employee --hive-import --hive-table empleados -m 1 --mysql-delimiters
